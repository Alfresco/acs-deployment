# Alfresco Content Services Helm Deployment with external Hazelcast cluster

- [Alfresco Content Services Helm Deployment with external Hazelcast
  cluster](#alfresco-content-services-helm-deployment-with-external-hazelcast-cluster).
  - [Prerequisites](#prerequisites)
  - [Warning](#warning)
  - [Step by step guide](#step-by-step-guide)
  - [Tests](#tests)

## Prerequisites

- Having installed either of:
  - [Rancher for Desktop](https://rancherdesktop.io/). Includes kubectl and
    Helm, ready to use right after installation.
  - [Docker for Desktop](https://docs.docker.com/desktop/). Requires separate
    install of kubectl and Helm.

## Warning

  This feautre won't be showing proper server details on Alfresco UI in
  Repository Server Clustering under section Cluster Members. Alfresco
  repository pod will be still present after scaling it down. Pod with new name
  will be added to list but the older will still remain. To verify that the
  solution is working you can check managment center or do manual test described
  in [Tests](#tests)

## Step by step guide

1. Deploy alfresco-content-services. See [desktop
   deployment](../desktop-deployment.md) section. Remember to stick to one
   namespace in next steps.

2. Prepare a valid Hazelcast xml configuration. First step in this [Alfresco
   docs](https://docs.alfresco.com/content-services/latest/admin/cluster/#set-up-repository-clustering-via-external-hazelcast).
   If you want to use config generated by us skip to 7th step.
    - Example file:
      [caches.properties](https://github.com/Alfresco/alfresco-community-repo/blob/master/repository/src/main/resources/alfresco/caches.properties)

3. Replace the `<cluster-name>` within the `alfresco-hazelcast-config.xml`file
   with a secure value that is hard to guess. For all intents and purposes this
   field  should be treated as a password, as if matching it allows
   client-server / member-member connection.

4. Add appropriate configation for kubernetes autodiscovery feature and [rest
   api
   groups](https://docs.hazelcast.com/hazelcast/5.3/maintain-cluster/rest-api#using-rest-endpoint-groups)
   depending on your choise.

    ```xml
    <network>
        <join>
            <kubernetes enabled="true">
                <service-name>hazelcast</service-name>
            </kubernetes>
        </join>
        <rest-api enabled="true">
            <endpoint-group name="CLUSTER_READ" enabled="true"/>
            <endpoint-group name="CLUSTER_WRITE" enabled="true"/>
            <endpoint-group name="HEALTH_CHECK" enabled="true"/>
            <endpoint-group name="HOT_RESTART" enabled="true"/>
            <endpoint-group name="WAN" enabled="true"/>
            <endpoint-group name="DATA" enabled="true"/>
        </rest-api>
    </network>
    <jet enabled="true">
    </jet>
    ```

5. Accoringly change the values for managment center. Put internal network
   address of your k8s.

    ```xml
    <management-center data-access-enabled="true">
        <trusted-interfaces>
            <interface>10.42.0.*</interface>
        </trusted-interfaces>
    </management-center>
    ```

6. Now preparation of the xml configuration is finished. Next step is to create
   configmap manifest. Add below section to newly created file:

    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: manual-hazelcast-configuration
      labels:
        app.kubernetes.io/name: hazelcast
    data:
      hazelcast.xml: |
    ```

    Now copy generated xml configuration into manifest file. (Remember to
    corectly indent copied text)

7. Deploy created [manifest](external-hazelcast_files/configmap-hazelcast.yaml)

    ```bash
    kubectl apply -f configmap-hazelcast.yaml 
    ```

8. Deploy hazelcast with [values](external-hazelcast_files/hazelcast.yaml)
   presented below

    ```yaml
    hazelcast:
      javaOpts: -Dhazelcast.config=/data/hazelcast/hazelcast.xml
      existingConfigMap: manual-hazelcast-configuration
    mancenter:
      enabled: true
        yaml:
          hazelcast-client:
            cluster-name: test
    ```

    ```bash
    helm repo add hazelcast https://hazelcast-charts.s3.amazonaws.com/
    helm repo update
    helm install hazelcast hazelcast/hazelcast -f hazelcast.yaml
    ```

9. Now change the config of alfresco-repository by adding another [values
   file](external-hazelcast_files/acs-hazelcast.yaml). In below file specify
   properties that will make repository use external hazelcast cluster deployed
   in previous step. Remember to accordingly change the values if needed.

    ```yaml
    alfresco-repository:
      replicaCount: 3
      config:
        repository:
          additionalGlobalProperties: 
            alfresco.hazelcast.embedded: false
            alfresco.hazelcast.client.address: hazelcast:5701
            alfresco.cluster.name: test
    ```

    ```bash
    helm upgrade acs helm/alfresco-content-services \    
    --values local-dev-values.yaml \
    --set global.search.sharedSecret=$(openssl rand -hex 24) \
    --atomic \
    --timeout 10m0s \
    --values acs-hazelcast.yaml
    ```

## Tests

1. Make sure that you have installed fresh environment. Do not login into
   Alfresco UI yet.

2. Add ubuntu deployment.

    ```bash
    helm repo add open https://simonmisencik.github.io/helm-charts
    helm repo update
    helm install testing open/ubuntu
    ```

3. Get into a shell of container created by previous commands.

    ```bash
    kubectl exec --stdin --tty testing-ubuntu -- /bin/bash
    ```

4. Execute curl command against all hazelcast nodes.

    ```bash
    curl http://hazelcast-0.hazelcast.default.svc.cluster.local:5701/hazelcast/rest/maps/cache.usernameToTicketIdCache/admin -o -
    ```

    ```bash
    curl http://hazelcast-1.hazelcast.default.svc.cluster.local:5701/hazelcast/rest/maps/cache.usernameToTicketIdCache/admin -o -
    ```

    ```bash
    curl http://hazelcast-2.hazelcast.default.svc.cluster.local:5701/hazelcast/rest/maps/cache.usernameToTicketIdCache/admin -o -
    ```

    You should get empty response.

5. Login into [Alfresco
   UI](http://localhost/alfresco/s/enterprise/admin/admin-clustering) and after
   that repeat commands from previous step. You should see resposne from all
   nodes simillar to this:

    ```bash
    ????????admin????(92023d8ea10294a2e32b238963e67d8013342e30
    ```
